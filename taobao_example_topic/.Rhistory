graph <- delete.edges(graph, E(graph)[ weight < minval])
E(graph)$edge.width <- E(graph)$weight*17
E(graph)$color <- "blue"
V(graph)$color <- ifelse(grepl("^\\d+$", V(graph)$name), "grey75", "orange")
V(graph)$frame.color <- NA
V(graph)$label <- ifelse(grepl("^\\d+$", V(graph)$name), paste("topic", V(graph)$name), gsub("_", "\n", V(graph)$name))
V(graph)$size <- c(rep(10, nrow(topic_mat)), colSums(topic_mat) * 20)
V(graph)$label.color <- ifelse(grepl("^\\d+$", V(graph)$name), "red", "grey30")
par(mar=c(0, 0, 3, 0))
set.seed(365)
plot.igraph(graph, edge.width = E(graph)$edge.width,
vertex.color = adjustcolor(V(graph)$color, alpha.f = .4))
title("Topic & Document Relationships", cex.main=.8)
lda_model %>%
topicmodels2LDAvis() %>%
LDAvis::serVis()
ls()
rm(list = ls())
gc()
getwd()
gc()
ls()
getwd()
require(devtools)
library(devtools)
install_github("vanatteveldt/topicbrowser")
install.packages("topicmodels")
library(topicmodels)
data(sotu)
library(topicbrowser)
data(sotu)
str(sotu)
ls()
str(sotu.dtm)
head(sotu.dtm)
sotu.dtm
class(sotu.dtm)
sotu.dtm$i
str(sotu.dtm$i)
class(sotu.dtm$i)
sotu.dtm$v
unique(sotu.dtm$v)
ls()
tokens = tokens[order(tokens$aid, tokens$id), ]
str(sotu.tokens)
head(sotu.tokens)
tail(sotu.tokens)
tokens = tokens[order(sotu.tokens$aid, sotu.tokens$id), ]
tokens = sotu.tokens[order(sotu.tokens$aid, sotu.tokens$id), ]
tokens
class(m)
ls()
head(tokens)
str(tokens)
ls()
head(sotu.meta)
str(sotu.meta)
class(sotu.lda_model)
class(sotu.lda_model)
colnames(tokens)
colnames(sotu.meta)
output = createTopicBrowser(sotu.lda_model,
tokens$lemma, tokens$aid, words=tokens$word,
meta=sotu.meta)
library(markdown)
result = rpubsUpload("Example topic browser", output)
output
output = createTopicBrowser(sotu.lda_model,
tokens$lemma, tokens$aid, words=tokens$word,
meta=sotu.meta)
sotu.lda_model
tokens$lemma
tokens$aid
tokens$word
sotu.meta
output = createTopicBrowser(sotu.lda_model,
tokens$lemma, tokens$aid, words=tokens$word,
meta=sotu.meta)
library(topicbrowser)
library(topicmodels)
data(sotu)
tokens = sotu.tokens[order(sotu.tokens$aid, sotu.tokens$id), ]
class(sotu.lda_model)
head(tokens)
head(sotu.meta)
output = createTopicBrowser(sotu.lda_model,
tokens$lemma, tokens$aid, words=tokens$word,
meta=sotu.meta)
output
head(presidential_debates_2012,10)
data(presidential_debates_2012)
require(pacman)
data(presidential_debates_2012)
require(gofastr)
require(gofastr)
data(presidential_debates_2012)
head(presidential_debates_2012,20)
names(presidential_debates_2012)
unique(presidential_debates_2012$person)
unique(presidential_debates_2012$tot)
unique(presidential_debates_2012$time)
unique(presidential_debates_2012$role)
unique(presidential_debates_2012$dialogue)
head(presidential_debates_2012,3)
a <- presidential_debates_2012 %>%
with(gofastr::q_dtm_stem(dialogue, paste(person, time, sep = "_")))
pacman::p_load_gh("trinker/gofastr")
pacman::p_load(tm, topicmodels, dplyr, tidyr, igraph, devtools, LDAvis, ggplot2)
a <- presidential_debates_2012 %>%
with(gofastr::q_dtm_stem(dialogue, paste(person, time, sep = "_")))
class(a)
str(a)
str(presidential_debates_2012)
dim(presidential_debates_2012)
7052/2912
install.packages("SnowballC")
install.packages("NLP")
install.packages("NLP")
install.packages("NLP")
install.packages("NLP")
install.packages("NLP")
install.packages("NLP")
install.packages("NLP")
install.packages("NLP")
install.packages("NLP")
install.packages("NLP")
library(SnowballC)
library(SnowballC)
library(SnowballC)
library(NLP)
library(tm)
library(SnowballC)
vignette("tm")
reut21578 <- system.file("texts", "wiki_sentence", package = "tm")
reut21578
reuters <- Corpus(DirSource(reut21578), readerControl = list(language = "english"))
reut21578 <- system.file("texts", "wiki_sentence", package = "tm")
reuters <- Corpus(DirSource(reut21578), readerControl = list(language = "english"))
reut21578
writeCorpus(reuters)
install.packages("SnowballC")
drv=dbDriver('Oracle')
options(java_home="C:\\Program Files\\Java\\jdk1.7.0_79\\jre")
Sys.setenv(PATH='C:\\Program Files\\Java\\jdk1.7.0_79\\jre\\bin\\server')
library(rJava)
library(Rdouban)
library(Rdouban)
ibrary(rJava)
library(rJava)
options(java_home="C:\\Program Files\\Java\\jdk1.7.0_79\\jre")
Sys.setenv(PATH='C:\\Program Files\\Java\\jdk1.7.0_79\\jre\\bin\\server')
install.packages("Rwordseg", repos="http://R-Forge.R-project.org")
library(Rdouban)
library(rJava)
options(java_home="C:\\Program Files\\Java\\jdk1.7.0_79\\jre")
Sys.setenv(PATH='C:\\Program Files\\Java\\jdk1.7.0_79\\jre\\bin\\server')
install.packages("Rwordseg", repos="http://R-Forge.R-project.org")
library(Rdouban)
library(rJava)
library(rJava)
options(java_home="C:\\Program Files (x86)\\Java\\jdk1.7.0_79\\jre")
Sys.setenv(PATH='C:\\Program Files (x86)\\Java\\jdk1.7.0_79\\jre\\bin\\server')
install.packages("Rwordseg", repos="http://R-Forge.R-project.org")
library(Rwordseg)
library(Rwordseg)
rm(list = listls())
rm(list = ls())
ls()
gc)()
gc)()
gc()
library(jiebaR)
library(jiebaRD)
library(jiebaR)
library(jiebaRD)
cutter <- worker(bylines = T,
user = "lda/UsrWords.dict",
stop_word = "lda/stopWords.txt")
comments_seg <- cutter["./lda/Mobilecomments.txt"]
comments_segged<- readLines(comments_seg,encoding="UTF-8")
str(comments_segged)
comments_seg <- cutter["./lda/hlm/hlm.txt"]
comments_segged<- readLines(comments_seg,encoding="UTF-8")
str(comments_segged)
class(comments_segged)
comments <- as.list(comments_segged) #将向量转化为列表
doc.list <- strsplit(as.character(comments),split=" ")
#将每行文本，按照空格分开，每行变成一个词向量，储存在列表里
term.table <- table(unlist(doc.list))
# 这里有两步，unlist用于统计每个词的词频；
# table把结果变成一个交叉表式的factor，原理类似python里的词典，key是词，value是词频
term.table <- sort(term.table, decreasing = TRUE) #按照词频降序排列
del <- term.table < 5| nchar(names(term.table))<2   #把不符合要求的筛出来
term.table <- term.table[!del]   #去掉不符合要求的
vocab <- names(term.table)    #创建词库
get.terms <- function(x) {
index <- match(x, vocab)  # 获取词的ID
index <- index[!is.na(index)]  #去掉没有查到的，也就是去掉了的词
rbind(as.integer(index - 1), as.integer(rep(1, length(index))))   #生成上图结构
}
documents <- lapply(doc.list, get.terms)
str(documents)
str(vocab)
class(presidential_debates_2012)
stopwords <- readLines("lda/stopWords.txt",encoding="UTF-8")
str(stopwords)
text <-  readLines("lda/hlm/hlm.txt",encoding="UTF-8")
str(text)
require(gofastr)
require(gofastr)
data(presidential_debates_2012)
str(presidential_debates_2012)
presidential_debates_2012 %>%
with(gofastr::q_dtm_stem(dialogue, paste(person, time, sep = "_")))
if (!require("pacman")) install.packages("pacman")
pacman::p_load_gh("trinker/gofastr")
pacman::p_load(tm, topicmodels, dplyr, tidyr, igraph, devtools, LDAvis, ggplot2)
presidential_debates_2012 %>%
with(gofastr::q_dtm_stem(dialogue, paste(person, time, sep = "_")))
text %>%
with(gofastr::q_dtm_stem())
class(text)
str(text)
text <-  readLines("lda/hlm/hlm.txt",encoding="UTF-8") %>%
as.list %>%
as.character %>%
trsplit(split=" ")
text <-  readLines("lda/hlm/hlm.txt",encoding="UTF-8") %>%
as.list %>%
as.character %>%
strsplit(split=" ")
class(text)
str(text)
text <-  readLines("lda/hlm/hlm.txt",encoding="UTF-8") %>%
as.list %>%
as.character %>%
strsplit(split=" ") %>%
as.data.frame()
class(presidential_debates_2012)
str(documents)
length(documents)
documents[1]
documents[2]
documents[3]
documents[4]
documents[5]
documents[6]
documents[7]
documents[8]
documents[9]
documents[10]
documents[11]
documents[12]
documents[13]
documents[14]
head(text)
library(SnowballC)
library(NLP)
library(tm)
reut21578 <- system.file("texts", "wiki_sentence", package = "tm")
reuters <- Corpus(DirSource(reut21578), readerControl = list(language = "english"))
reut21578 <- system.file("texts", "wiki_sentence", package = "tm")
reut21578
reuters <- Corpus(DirSource(reut21578), readerControl = list(language = "english"))
library(SnowballC)
library(NLP)
library(tm)
reut21578 <- system.file("texts", "wiki_sentence", package = "tm")
reut21578
DirSource(reut21578)
getwd()
patent<-readLines('D:\\Workspace\\Rsession\\lda\\Mobilecomments.txt',encoding='UFT-8')
patent
patent<-readLines('D:\\Workspace\\Rsession\\lda\\Mobilecomments.txt',encoding='UFT-8')
patent
patent<-readLines('D:\\Workspace\\Rsession\\lda\\Mobilecomments.txt',encoding='GBK')
patent
patent <- readLines('D:\\Workspace\\Rsession\\lda\\Mobilecomments.txt',encoding='UTF-8')
patent
patent <- readLines('D:\\Workspace\\Rsession\\lda\\Mobilecomments.txt',encoding='UTF-8')
getwd()
patent <- readLines('lda/Mobilecomments.txt',encoding='UTF-8')
patent <- readLines('lda/Mobilecomments.txt',encoding='UTF-8')
stopwords <- readLines("lda/stopWords.txt",encoding="UTF-8")
stopwords
patent <- readLines('lda/Mobilecomments.txt',encoding="UTF-8")
patent <- readLines("lda/Mobilecomments.txt",encoding="UTF-8")
patent
patent <- readLines('lda/Mobilecomments.txt',encoding="UTF-8")
patent
patent <- readLines('lda/Mobilecomments.txt',encoding="UTF-8")
patent
patent <- readLines('lda/Mobilecomments.txt',encoding='UTF-8')
patent
patentname_abstract1<-Corpus(VectorSource(patent))
library(SnowballC)
library(NLP)
library(tm)
patentname_abstract1<-Corpus(VectorSource(patent))
patentname_abstract1
inspect(patentname_abstract1)
patentname_abstract2.2<-sapply(patentname_abstract1, extractNoun, USE.NAMES=F)
install.packages("rpaoding", repos="http://R-Forge.R-project.org")
library(rpaoding)
Sys.setenv(JAVA_HOME='C:/Program Files (x86)/Java/jdk1.7.0_79/jre')#
install.packages("rpaoding", repos="http://R-Forge.R-project.org")
library(Rwordseg)
library(rJava)
library(rJava)
library(rJava)
Sys.setenv(JAVA_HOME='C:/Program Files (x86)/Java/jdk1.7.0_79/')
library(rJava)
Sys.setenv(JAVA_HOME="C:/Program Files (x86)/Java/jdk1.7.0_79/")#
library(rJava)
Sys.setenv(JAVA_HOME="C:/Program Files/Java/jdk1.8.0_121")#
library(rJava)
library(Rwordseg)
install.packages("rpaoding", repos="http://R-Forge.R-project.org")
library(rpaoding)
library(rpaoding)
library(Rwordseg)
help(rpaoding)
library(rpaoding)
paoding("敢问路在何方，路在脚下。")
library(rpaoding)
paoding("敢问路在何方，路在脚下。")
library(tm)
getwd()
txt <- system.file("texts", "txt", package = "tm")
txt
(ovid <- Corpus(DirSource(txt),readerControl = list(language = "lat")))
ovid
str(ovid)
docs <- c("This is a text.", "This another one.")
Corpus(VectorSource(docs))
reut21578 <- system.file("texts", "crude", package = "tm")
reut21578
reuters <- Corpus(DirSource(reut21578),readerControl = list(reader = readReut21578XML))
inspect(ovid[1:2])
str(reuters)
length(reuters)
inspect(reuters[1:2])
inspect(reuters[1])
class(reuters)
class(ovid)
inspect(ovid[1:2])
inspect(reuters[1])
inspect(reuters[[1]])
reuters1 <- tm_map(reuters, as.PlainTextDocument)  #
reuters1 <- tm_map(reuters, as.PlainTextDocument)
GreekShippingContent <- "The Greek administration is coming under increasing pressure over it foot-dragging regarding its meeting international convention deadlines, especially when it relies on classification societies as an Recognised Organisation (RO) on its behalf. "
GreekShippingContent0 <-  Corpus(VectorSource(GreekShippingContent))
tm_map(GreekShippingContent0, PlainTextDocument)
reuters1 <- tm_map(reuters, PlainTextDocument)
reuters1
inspect(reuters1[[1]])
inspect(reuters[[1]])
reuters2 <- tm_map(reuters1, stripWhitespace)
inspect(reuters2[[1]])
reuters3 <- tm_map(reuters2, tolower)
inspect(reuters3[[1]])
reuters3 <- tm_map(reuters2, tolower)
inspect(reuters3[[1]])
reuters3
inspect(reuters3[[1]])
reuters4 <- tm_map(reuters3, removeWords, stopwords("english"))
inspect(reuters4[[1]])
class(reuters3)
class(reuters2)
inspect(reuters2[[1]])
inspect(reuters3[[1]])
ls()
rm(list = ls())
gc()
ls()
female_link <- 'https://s.taobao.com/search?q=女'
rawstring = readLines(female_link,encoding="UTF-8")
head(rawstring)
str(rawstring)
length(rawstring)
rawstring[1]
rawstring[14]
rawstring[4]
rawstring = paste0(rawstring,collapse = ' ')
head(rawstring)
length(rawstring)
str(rawstring)
s <- gsub('\\w','',rawstring,perl = T)
str(s)
length(s)
class(s)
head(s)
s1 <- gsub('[[:punct:]]',' ',s)
s1
ls()
rm(list = ls())
gc()
ls()
library(jiebaR)
library(wordcloud2)
#读取中文并去掉非中文及注音符号
readChineseWords<-function(path){
rawstring = readLines(path)
rawstring = paste0(rawstring,collapse = ' ')
s <- gsub(' ','',rawstring)
s <- gsub('\\w','',rawstring,perl = T)
s <- gsub('[[:punct:]]',' ',s)
return(s)
}
uri <- 'taobao/test.txt'
Rt<-readChineseWords(uri)
str(Rt)
class(Rt)
readChineseWords<-function(path){
rawstring = readLines(path,encoding="UTF-8")
rawstring = paste0(rawstring,collapse = ' ')
s <- gsub(' ','',rawstring)
s <- gsub('\\w','',rawstring,perl = T)
s <- gsub('[[:punct:]]',' ',s)
return(s)
}
Rt<-readChineseWords(uri)
str(Rt)
Rt<-gsub(' ','',Rt)
Rt<-gsub(' ','',Rt)
str(Rt)
words<-worker(stop_word = 'taobao/stop.txt')
words<-worker(stop_word = 'taobao/stops.txt')
head(Rt)
new_user_word(words,c('子婴','n','秦始皇','n'))
seg<-segment(Rt,words)
str(seg)
keys<-worker('keywords',topn=30)
keys
v<-vector_keywords(seg,keys)
v
df<-stack(v)
df
class(v)
typeof(df$ind)
df$ind<-as.numeric(as.character(df$ind))
typeof(df$ind)
wordcloud2(df)
library(jiebaR)
library(jiebaRD)
library(lda)
library(LDAvis)
library(jiebaR)
library(jiebaRD)
library(lda)
library(LDAvis)
setwd("taobao/")
getwd()
cutter <- worker(bylines = T,stop_word = "./stops.txt")
comments_seg <- cutter["test.txt"]
comments_segged <- readLines(comments_seg,encoding = "UTF-8")
comments_segged
length(comments_segged)
comments <- as.list(comments_segged)
doc.list <- strsplit(as.character(comments),split = " ")
term.table <- table(unlist(doc.list))
term.table
term.table <- sort(term.table, decreasing = TRUE)
term.table
del <- term.table < 5| nchar(names(term.table)) < 2
term.table <- term.table[!del]
vocab <- names(term.table)
term.table
vocab <- names(term.table)
vocab
get.terms <- function(x) {
index <- match(x,vocab)
index <- index[!is.na(index)]
rbind(as.integer(index - 1), as.integer(rep(1, length(index))))
}
documents <- lapply(doc.list, get.terms)
doc.list
documents
documents[[1]]
doc.list[[1]]
vocab[290]
vocab[291]
documents[[1]]
doc.list[1]
vocab[291]
vocab[628]
K <- 8
G <- 5000
alpha <- 0.10
eta <- 0.02
set.seed(357)
fit <- lda.collapsed.gibbs.sampler(documents = documents,
K = K,
vocab = vocab,
num.iterations = G,
alpha = alpha,
eta = eta,
initial = NULL,
burnin = 0,
compute.log.likelihood = TRUE)
theta <- t(apply(fit$document_sums + alpha, 2, function(x) x/sum(x)))
phi <- t(apply(t(fit$topics) + eta, 2, function(x) x/sum(x)))
term.frequency <- as.integer(term.table)
doc.length <- sapply(documents, function(x) sum(x[2, ]))
json <- createJSON(phi = phi,
theta = theta,
doc.length = doc.length,
vocab = vocab,
term.frequency = term.frequency)
serVis(json, out.dir = './vis', open.browser = FALSE)
writeLines(iconv(readLines("./vis/lda.json"), from = "GBK", to = "UTF8"),
file("./vis/lda.json", encoding="UTF-8"))
